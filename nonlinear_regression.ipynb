{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fcfaf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with sgd optimizer, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 1.6699517965316772\n",
      "Epoch 100, Loss: 1.185174584388733\n",
      "Training with sgd optimizer, Learning Rate: 0.005\n",
      "Epoch 0, Loss: 1.2234455347061157\n",
      "Epoch 100, Loss: 1.1859381198883057\n",
      "Training with sgd optimizer, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 1.57686448097229\n",
      "Epoch 100, Loss: 1.1376088857650757\n",
      "Training with sgd optimizer, Learning Rate: 0.02\n",
      "Epoch 0, Loss: 1.2858954668045044\n",
      "Epoch 100, Loss: 1.163874864578247\n",
      "Optimal Learning Rate for sgd: 0.005\n",
      "Training with adam optimizer, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 1.2446575164794922\n",
      "Epoch 100, Loss: 1.1583644151687622\n",
      "Training with adam optimizer, Learning Rate: 0.005\n",
      "Epoch 0, Loss: 1.3413984775543213\n",
      "Epoch 100, Loss: 1.0970300436019897\n",
      "Training with adam optimizer, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 1.4299474954605103\n",
      "Epoch 100, Loss: 1.1471378803253174\n",
      "Training with adam optimizer, Learning Rate: 0.02\n",
      "Epoch 0, Loss: 1.5571529865264893\n",
      "Epoch 100, Loss: 1.1547386646270752\n",
      "Optimal Learning Rate for adam: 0.02\n",
      "Extended Training with sgd optimizer, Learning Rate: 0.005\n",
      "Epoch 0, Loss: 1.24507737159729\n",
      "Epoch 100, Loss: 1.1703956127166748\n",
      "Epoch 200, Loss: 1.1108373403549194\n",
      "Epoch 300, Loss: 1.123806357383728\n",
      "Epoch 400, Loss: 1.159923791885376\n",
      "Epoch 500, Loss: 1.1201903820037842\n",
      "Epoch 600, Loss: 1.1439712047576904\n",
      "Epoch 700, Loss: 1.1192913055419922\n",
      "Epoch 800, Loss: 1.1345462799072266\n",
      "Epoch 900, Loss: 1.158700942993164\n",
      "Epoch 1000, Loss: 1.1548740863800049\n",
      "Epoch 1100, Loss: 1.1006251573562622\n",
      "Epoch 1200, Loss: 1.1470386981964111\n",
      "Epoch 1300, Loss: 1.1236652135849\n",
      "Epoch 1400, Loss: 1.1409000158309937\n",
      "Epoch 1500, Loss: 1.1262171268463135\n",
      "Epoch 1600, Loss: 1.106614112854004\n",
      "Epoch 1700, Loss: 1.0620312690734863\n",
      "Epoch 1800, Loss: 1.1003323793411255\n",
      "Epoch 1900, Loss: 1.0870522260665894\n",
      "Epoch 2000, Loss: 1.1251004934310913\n",
      "Epoch 2100, Loss: 1.1755422353744507\n",
      "Epoch 2200, Loss: 1.1188448667526245\n",
      "Epoch 2300, Loss: 1.1751728057861328\n",
      "Epoch 2400, Loss: 1.1027992963790894\n",
      "Epoch 2500, Loss: 1.132012128829956\n",
      "Epoch 2600, Loss: 1.1477769613265991\n",
      "Epoch 2700, Loss: 1.1234517097473145\n",
      "Epoch 2800, Loss: 1.129941463470459\n",
      "Epoch 2900, Loss: 1.1242058277130127\n",
      "Epoch 3000, Loss: 1.0656076669692993\n",
      "Epoch 3100, Loss: 1.1641244888305664\n",
      "Epoch 3200, Loss: 1.1205724477767944\n",
      "Epoch 3300, Loss: 1.1322444677352905\n",
      "Epoch 3400, Loss: 1.1517202854156494\n",
      "Epoch 3500, Loss: 1.159706473350525\n",
      "Epoch 3600, Loss: 1.1394776105880737\n",
      "Epoch 3700, Loss: 1.06022047996521\n",
      "Epoch 3800, Loss: 1.1310741901397705\n",
      "Epoch 3900, Loss: 1.1874336004257202\n",
      "Epoch 4000, Loss: 1.1528526544570923\n",
      "Epoch 4100, Loss: 1.114837884902954\n",
      "Epoch 4200, Loss: 1.1550911664962769\n",
      "Epoch 4300, Loss: 1.1636816263198853\n",
      "Epoch 4400, Loss: 1.1365411281585693\n",
      "Epoch 4500, Loss: 1.1679420471191406\n",
      "Epoch 4600, Loss: 1.151694416999817\n",
      "Epoch 4700, Loss: 1.0993572473526\n",
      "Epoch 4800, Loss: 1.073533535003662\n",
      "Epoch 4900, Loss: 1.118418574333191\n",
      "Epoch 5000, Loss: 1.1927262544631958\n",
      "Epoch 5100, Loss: 1.124571681022644\n",
      "Epoch 5200, Loss: 1.149970531463623\n",
      "Epoch 5300, Loss: 1.1433112621307373\n",
      "Epoch 5400, Loss: 1.1228852272033691\n",
      "Epoch 5500, Loss: 1.166557788848877\n",
      "Epoch 5600, Loss: 1.0635584592819214\n",
      "Epoch 5700, Loss: 1.119956135749817\n",
      "Epoch 5800, Loss: 1.1351817846298218\n",
      "Epoch 5900, Loss: 1.1346932649612427\n",
      "Epoch 6000, Loss: 1.164781928062439\n",
      "Epoch 6100, Loss: 1.1220227479934692\n",
      "Epoch 6200, Loss: 1.1116410493850708\n",
      "Epoch 6300, Loss: 1.0863455533981323\n",
      "Epoch 6400, Loss: 1.132554054260254\n",
      "Epoch 6500, Loss: 1.113818883895874\n",
      "Epoch 6600, Loss: 1.1583181619644165\n",
      "Epoch 6700, Loss: 1.140176773071289\n",
      "Epoch 6800, Loss: 1.1505711078643799\n",
      "Epoch 6900, Loss: 1.1512501239776611\n",
      "Epoch 7000, Loss: 1.1230463981628418\n",
      "Epoch 7100, Loss: 1.1288737058639526\n",
      "Epoch 7200, Loss: 1.0952867269515991\n",
      "Epoch 7300, Loss: 1.1355023384094238\n",
      "Epoch 7400, Loss: 1.1442538499832153\n",
      "Epoch 7500, Loss: 1.1532411575317383\n",
      "Epoch 7600, Loss: 1.1381208896636963\n",
      "Epoch 7700, Loss: 1.1103196144104004\n",
      "Epoch 7800, Loss: 1.0858473777770996\n",
      "Epoch 7900, Loss: 1.1414012908935547\n",
      "Epoch 8000, Loss: 1.1432603597640991\n",
      "Epoch 8100, Loss: 1.2124789953231812\n",
      "Epoch 8200, Loss: 1.0722825527191162\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115261/4148094981.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moptimizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimal_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Extended Training with {optimizer_name} optimizer, Learning Rate: {lr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mfinal_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_115261/4148094981.py\u001b[0m in \u001b[0;36mtrain_transformer\u001b[0;34m(optimizer_name, learning_rate, max_epochs)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_context_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch import nn\n",
    "def in_context_loss(model, Z, y):\n",
    "    N = Z.shape[1]-1\n",
    "    d = Z.shape[2]-1\n",
    "    output = model(Z)\n",
    "    diff = output[:,N,d]+y\n",
    "    loss = ((diff)**2).mean()\n",
    "    return loss\n",
    "def attention(P,Q,Z, activation = None):\n",
    "    B= Z.shape[0]\n",
    "    N = Z.shape[1]-1\n",
    "    d = Z.shape[2]-1\n",
    "    P_full =  torch.cat([P,torch.zeros(1,d).to(device)],dim=0)\n",
    "    P_full =  torch.cat([P_full,torch.zeros(d+1,1).to(device)],dim=1)\n",
    "    P_full = P_full.clone()\n",
    "    P_full[d, d] = 1\n",
    "    Q_full = torch.cat([Q, torch.zeros(1,d).to(device)],dim=0)\n",
    "    Q_full = torch.cat([Q_full, torch.zeros(d+1,1).to(device)],dim=1)\n",
    "    A = torch.eye(N+1).to(device)\n",
    "    A[N,N] = 0\n",
    "    Attn = torch.einsum('BNi, ij, BMj -> BNM', (Z,Q_full,Z))\n",
    "    if activation is not None:\n",
    "        Attn = activation(Attn)\n",
    "    key = torch.einsum('ij, BNj -> BNi', (P_full,Z))\n",
    "    Output = torch.einsum('BNM,ML, BLi -> BNi', (Attn,A,key))\n",
    "    return Output /N\n",
    "def generate_data_sine(N=10, B=1000, d=1):\n",
    "    # Sample amplitude a and phase œÅ for each task\n",
    "    a = torch.FloatTensor(B, 1).uniform_(1, 2).cuda()\n",
    "    rho = torch.FloatTensor(B, 1).uniform_(np.pi/2, np.pi).cuda()\n",
    "    # Sample inputs x uniformly between -5 and 5\n",
    "    X = torch.FloatTensor(B, N, d).uniform_(-5, 5).cuda()\n",
    "    X_test = torch.FloatTensor(B, 1, d).uniform_(-5, 5).cuda()\n",
    "    y = a.unsqueeze(1) * torch.sin(rho.unsqueeze(1) + X)\n",
    "    y_test = a.unsqueeze(1) * torch.sin(rho.unsqueeze(1) + X_test)\n",
    "    # Combine X and y for the full dataset\n",
    "    X_comb = torch.cat([X, X_test], dim=1)\n",
    "    y_comb = torch.cat([y, torch.zeros(B, 1, 1).cuda()], dim=1)\n",
    "    Z = torch.cat([X_comb, y_comb], dim=2)\n",
    "    return Z.to('cuda'), y_test.squeeze(1).to('cuda')\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim-1))\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(output_dim-1, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, output_dim-1)\n",
    "        )\n",
    "    def forward(self, Z):\n",
    "        X = Z[:,:,0].unsqueeze(2)\n",
    "        out = self.embed(X) + self.mlp2(self.embed(X))\n",
    "        out = torch.cat([out,Z[:,:,1].unsqueeze(2)],dim=2)\n",
    "        return out\n",
    "        #return self.layers(x)\n",
    "class Transformer_MLP(nn.Module):\n",
    "    def __init__(self, n_layer, n_head, d, var):\n",
    "        super(Transformer_MLP, self).__init__()\n",
    "        # Ensure the MLP is initialized with the correct dimensions\n",
    "        hidden_size=160\n",
    "        output_dim=40\n",
    "        self.mlp = MLP(input_dim=1, hidden_dim=hidden_size, output_dim=output_dim)  # Adjust hidden_dim as needed\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.register_parameter('allparam', nn.Parameter(torch.zeros(n_layer, n_head, 2, output_dim-1, output_dim-1)))\n",
    "        with torch.no_grad():\n",
    "            self.allparam.normal_(0, var)\n",
    "    def forward(self, Z):\n",
    "        B, N, _ = Z.shape\n",
    "        # Apply MLP to each token\n",
    "        #for i in range(N):\n",
    "        #Z_new = torch.zeros([B,N,20]).cuda()#Z.clone()\n",
    "        #Z_new[:, i, :] = self.mlp(Z[:, i, :]) #+ Z[:, i, :]\n",
    "        Z = self.mlp(Z)\n",
    "        #Z = Z_new\n",
    "        #print(Z.shape)\n",
    "        # Apply self-attention layers\n",
    "        for i in range(self.n_layer):\n",
    "            Zi = Z\n",
    "            residues = 0\n",
    "            for j in range(self.n_head):\n",
    "                Pij = self.allparam[i, j, 0, :, :]\n",
    "                Qij = self.allparam[i, j, 1, :, :]\n",
    "                residues = residues + attention(Pij, Qij, Zi)\n",
    "            Z = Zi + residues\n",
    "        return Z\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_layer = 3  # number of layers of transformer\n",
    "d = 1        # dimension for sine data\n",
    "n_head = 1   # 1-headed attention\n",
    "B = 1000     # minibatch size for sine data\n",
    "var = 0.0001 # initializations scale of transformer parameter\n",
    "max_iters = 10000  # Number of Iterations to run\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.002]\n",
    "# Function to run training\n",
    "def train_transformer(optimizer_name, learning_rate, max_epochs):\n",
    "    # Initialize model and optimizer\n",
    "    model = Transformer_MLP(n_layer, n_head, d, var).to(device)\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer_name == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.9))\n",
    "    losses = []\n",
    "    for epoch in range(max_epochs):\n",
    "        # Generate data\n",
    "        Z, y = generate_data_sine(N=100, B=B)\n",
    "        Z, y = Z.to(device), y.to(device)\n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        loss = in_context_loss(model, Z, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Record loss\n",
    "        losses.append(loss.item())\n",
    "        # Optionally print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "    return losses\n",
    "# Tuning learning rate\n",
    "tuning_epochs = 200\n",
    "optimal_lr = {}\n",
    "for optimizer_name in ['adam']:\n",
    "    best_lr = None\n",
    "    best_loss = float('inf')\n",
    "    for lr in learning_rates:\n",
    "        print(f\"Training with {optimizer_name} optimizer, Learning Rate: {lr}\")\n",
    "        losses = train_transformer(optimizer_name, lr, tuning_epochs)\n",
    "        avg_loss = np.mean(losses[-20:])  # Average loss over last epochs\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_lr = lr\n",
    "    optimal_lr[optimizer_name] = best_lr\n",
    "    print(f\"Optimal Learning Rate for {optimizer_name}: {best_lr}\")\n",
    "# Extended training with optimal learning rates\n",
    "final_losses = {}\n",
    "for optimizer_name, lr in optimal_lr.items():\n",
    "    print(f\"Extended Training with {optimizer_name} optimizer, Learning Rate: {lr}\")\n",
    "    losses = train_transformer(optimizer_name, lr, max_iters)\n",
    "    final_losses[optimizer_name] = losses\n",
    "# Plotting losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "for optimizer_name, losses in final_losses.items():\n",
    "    plt.plot(losses, label=f'{optimizer_name} LR={optimal_lr[optimizer_name]}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
