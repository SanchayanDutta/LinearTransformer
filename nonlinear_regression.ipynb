{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7soBJnRez5fP",
        "outputId": "956e3083-ea30-4d3b-f70b-c2e939dba357"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)  # PyTorch\n",
        "\n",
        "    # Ensures that PyTorch gets the same random numbers for CUDA as well, if using GPU.\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def attention(P,Q,Z, activation = None):\n",
        "    B= Z.shape[0]\n",
        "    N = Z.shape[1]-1\n",
        "    d = Z.shape[2]-1\n",
        "    A = torch.eye(N+1).to(device)\n",
        "    A[N,N] = 0\n",
        "    Attn = torch.einsum('BNi, ij, BMj -> BNM', (Z,Q,Z))\n",
        "    if activation is not None:\n",
        "        Attn = activation(Attn)\n",
        "    key = torch.einsum('ij, BNj -> BNi', (P,Z))\n",
        "    Output = torch.einsum('BNM,ML, BLi -> BNi', (Attn,A,key))\n",
        "    return Output /N\n",
        "\n",
        "\n",
        "\n",
        "class Transformer_MLP(nn.Module):\n",
        "    def __init__(self, n_layer, n_head, d, var, hidden_dim):\n",
        "        super(Transformer_MLP, self).__init__()\n",
        "        self.register_parameter('allparam', torch.nn.Parameter(torch.zeros(n_layer, n_head, 2, d+1, d+1)))\n",
        "        with torch.no_grad():\n",
        "            self.allparam.normal_(0,var)\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.mlp = nn.Sequential(nn.Linear(d, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim, d))\n",
        "\n",
        "    def forward(self, Z):\n",
        "        Z[:,:,:-1] = self.mlp(Z[:,:,:-1].clone())\n",
        "        for i in range(self.n_layer):\n",
        "            Zi = Z\n",
        "            residues = 0\n",
        "            # the forwarad map of each layer is given by F(Z) = Z + attention(Z)\n",
        "            for j in range(self.n_head):\n",
        "                Pij = self.allparam[i,j,0,:,:]\n",
        "                Qij = self.allparam[i,j,1,:,:]\n",
        "                residues = residues + attention(Pij,Qij,Zi)\n",
        "            Z = Zi + residues\n",
        "        return Z\n",
        "\n",
        "# evaluate the loss of model, given data (Z,y)\n",
        "def in_context_loss(model, Z, y):\n",
        "    N = Z.shape[1]-1\n",
        "    d = Z.shape[2]-1\n",
        "    output = model(Z)\n",
        "    diff = output[:,N,d]+y\n",
        "    loss = ((diff)**2).mean()\n",
        "    return loss\n",
        "\n",
        "def generate_data_mlp(N, B, d, randomMLP):\n",
        "    # Generate random input data\n",
        "    X = torch.FloatTensor(B, N, d).normal_(0, 1).to(device)\n",
        "    X_test = torch.FloatTensor(B, 1, d).normal_(0, 1).to(device)\n",
        "\n",
        "    # Additional transformations if mode is 'sphere' or 'gamma' [Similar to the existing generate_data function]\n",
        "\n",
        "    X_MLP = randomMLP(X.view(-1, d)).view(B, N, d)\n",
        "    X_test_MLP = randomMLP(X_test.view(-1, d)).view(B, 1, d)\n",
        "\n",
        "    W = torch.FloatTensor(B, d).normal_(0,1).cuda()\n",
        "    y = torch.einsum('bi,bni->bn', (W, X_MLP)).unsqueeze(2)\n",
        "    y_zero = torch.zeros(B,1,1).cuda()\n",
        "    y_test = torch.einsum('bi,bni->bn', (W, X_test_MLP)).squeeze(1)\n",
        "    X_comb= torch.cat([X,X_test],dim=1)\n",
        "    y_comb= torch.cat([y,y_zero],dim=1)\n",
        "    Z= torch.cat([X_comb,y_comb],dim=2)\n",
        "\n",
        "    return Z, y_test\n",
        "# Setup\n",
        "N=20\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_layer = 3  # number of layers of transformer\n",
        "d = 5        # dimension for the data\n",
        "n_head = 1   # 1-headed attention\n",
        "B = 2000     # minibatch size for sine data\n",
        "var = 0.1 # 0.0001 # initializations scale of transformer parameter\n",
        "clip = 1\n",
        "\n",
        "\n",
        "tuning_epochs = 400\n",
        "max_iters = 10000  # Number of Iterations to run\n",
        "hidden_dim = 10\n",
        "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
        "\n",
        "# Function to run training\n",
        "def train_transformer(optimizer_name, learning_rate, max_epochs, seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    randomMLP = nn.Sequential(nn.Linear(d, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim, d)).to(device)\n",
        "    for para in randomMLP.parameters():\n",
        "        para.requires_grad = False\n",
        "    # Initialize model and optimizer\n",
        "\n",
        "    # Instantiate model\n",
        "    model = Transformer_MLP(n_layer, n_head, d, var, 3* hidden_dim).to(device)\n",
        "\n",
        "    # Wrap the model for DataParallel\n",
        "    # if torch.cuda.device_count() > 1:\n",
        "    #     print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
        "    #     model = nn.DataParallel(model)\n",
        "\n",
        "    if optimizer_name == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer_name == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99))\n",
        "\n",
        "    # pipe output to log file\n",
        "    log_dir = 'log_nonlinear' \n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    filename = log_dir + f'/nonlinear_layer{n_layer}_N{N}_{optimizer_name}_lr{lr}_sd{seed}.pth'\n",
        "\n",
        "    losses = []\n",
        "    hist_list = list()\n",
        "    for epoch in range(max_epochs):\n",
        "        # Generate data\n",
        "        Z, y = generate_data_mlp(N=N, B=B, d=d, randomMLP=randomMLP)\n",
        "\n",
        "        # Move data to the correct device\n",
        "        Z, y = Z.to(device), y.to(device)\n",
        "\n",
        "        # Training step\n",
        "        optimizer.zero_grad()\n",
        "        loss = in_context_loss(model, Z, y)\n",
        "        loss.backward()\n",
        "        grads = []\n",
        "        for param in model.parameters():\n",
        "            grads.append(param.grad.clone().view(-1))\n",
        "        gradient = torch.cat(grads)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Record loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Optionally print progress\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()} GramNorm: {torch.norm(gradient).item()}')\n",
        "        hist_list.append(model.state_dict())\n",
        "    torch.save({'hist_list':hist_list}, filename)\n",
        "    return losses\n",
        "\n",
        "# Tuning learning rate\n",
        "\n",
        "optimal_lr = {\"adam\":0.01, \"sgd\":0.05}\n",
        "# for optimizer_name in ['adam','sgd']:\n",
        "#     best_lr = None\n",
        "#     best_loss = float('inf')\n",
        "#     for lr in learning_rates:\n",
        "#         print(f\"Training with {optimizer_name} optimizer, Learning Rate: {lr}\")\n",
        "#         losses = train_transformer(optimizer_name, lr, tuning_epochs)\n",
        "#         avg_loss = np.mean(losses[-20:])  # Average loss over last epochs\n",
        "#         if avg_loss < best_loss:\n",
        "#             best_loss = avg_loss\n",
        "#             best_lr = lr\n",
        "#     optimal_lr[optimizer_name] = best_lr\n",
        "#     print(f\"Optimal Learning Rate for {optimizer_name}: {best_lr}\")\n",
        "\n",
        "num_runs = 5\n",
        "final_losses = {}\n",
        "\n",
        "\n",
        "for optimizer_name, lr in optimal_lr.items():\n",
        "    print(f\"Extended Training with {optimizer_name} optimizer, Learning Rate: {lr}\")\n",
        "    all_losses = []\n",
        "    for seed in range(num_runs):\n",
        "        set_random_seed(seed)  # Function to set the random seed, define it as needed\n",
        "        losses = train_transformer(optimizer_name, lr, max_iters, seed)\n",
        "        all_losses.append(losses)\n",
        "\n",
        "    # Calculate the mean and standard deviation of the losses\n",
        "    mean_losses = np.mean(all_losses, axis=0)\n",
        "    std_losses = np.std(all_losses, axis=0)\n",
        "    final_losses[optimizer_name] = (mean_losses, std_losses, all_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig_dir = 'figures_nonlinear' \n",
        "os.makedirs(fig_dir, exist_ok=True)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1,figsize = (6, 6))\n",
        "epochs = range(len(mean_losses))  # Assuming all runs have the same number of epochs\n",
        "for optimizer_name, (mean_losses, std_losses, all_losses) in final_losses.items():\n",
        "    mean_losses = np.mean(np.log(all_losses), axis=0)\n",
        "    std_losses = np.std(np.log(all_losses), axis=0)\n",
        "    if optimizer_name == 'sgd':\n",
        "        ax.plot(epochs, mean_losses, color = 'black', lw=3, label='SGDM')\n",
        "        ax.fill_between(epochs, mean_losses - std_losses/2, mean_losses + std_losses/2, color='black', alpha=0.1)\n",
        "    if optimizer_name == 'adam':\n",
        "        ax.plot(epochs, mean_losses, color = 'red', lw=3, label='Adam')\n",
        "        ax.fill_between(epochs, mean_losses - std_losses/2, mean_losses + std_losses/2, color='red', alpha=0.1)\n",
        "    ax.set_xlabel('Iteration',fontsize=40)\n",
        "    ax.set_ylabel('Log Loss',fontsize=40)\n",
        "    ax.tick_params(axis='both', which='major', labelsize=30, width = 3, length = 10)\n",
        "    ax.tick_params(axis='both', which='minor', labelsize=20, width = 3, length = 5)\n",
        "    ax.legend(fontsize=30)\n",
        "    ax.spines[['right', 'top']].set_visible(False)\n",
        "    ax.spines['left'].set_linewidth(3)\n",
        "    ax.spines['bottom'].set_linewidth(3)\n",
        "    # ax.set_yscale('log')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_dir + '/loss_layer{}_N{}.pdf'.format(n_layer, N), dpi=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_dir = 'log_nonlinear' \n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "randomMLP = nn.Sequential(nn.Linear(d, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim, d)).to(device)\n",
        "model = Transformer_MLP(n_layer, n_head, d, var, 3* hidden_dim)\n",
        "model.to(device)\n",
        "\n",
        "# compute estimated true gradient using large batch\n",
        "B_large = B*100\n",
        "Z, y = generate_data_mlp(N=N, B=B_large, d=d, randomMLP=randomMLP)\n",
        "Z = Z.cuda()\n",
        "y = y.cuda()\n",
        "\n",
        "# redefine loss using newly sampled Z and y\n",
        "def eval_loss():\n",
        "    output = model(Z)\n",
        "    N= Z.shape[1]-1\n",
        "    diff = output[:,N,d]+y\n",
        "    loss = ((diff)**2).mean() \n",
        "    loss = loss \n",
        "    return loss\n",
        "\n",
        "loss = eval_loss()\n",
        "loss.backward()\n",
        "\n",
        "grads = []\n",
        "for param in model.parameters():\n",
        "    grads.append(param.grad.clone().view(-1))\n",
        "    param.grad.zero_()\n",
        "gradient = torch.cat(grads)\n",
        "\n",
        "noiseList = []\n",
        "n_sample = 100000\n",
        "for _ in range(n_sample):\n",
        "    # compute stochastic gradient\n",
        "    Z, y = generate_data_mlp(N=N, B=B, d=d, randomMLP=randomMLP)\n",
        "    Z = Z.cuda()\n",
        "    y = y.cuda()\n",
        "    \n",
        "    loss = in_context_loss(model, Z, y)\n",
        "    loss.backward()\n",
        "\n",
        "    stochastic_grads = []\n",
        "    for param in model.parameters():\n",
        "        stochastic_grads.append(param.grad.clone().view(-1))\n",
        "        param.grad.zero_()\n",
        "    stochastic_gradient = torch.cat(stochastic_grads)\n",
        "\n",
        "    noise = torch.norm(stochastic_gradient - gradient)\n",
        "    noiseList.append(noise.item())\n",
        "\n",
        "filename = log_dir + '/stochastic_gradient_noise_layer{}_N{}.pth'.format(n_layer,N)\n",
        "torch.save({'noiseList':noiseList}, filename)\n",
        "\n",
        "\n",
        "\n",
        "loaded_dict = torch.load(filename)\n",
        "noiseList = loaded_dict['noiseList']\n",
        "noiseArray = np.array(noiseList)\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "fig, ax = plt.subplots(1, 1,figsize = (6, 6))\n",
        "ax.hist(noiseList, bins=100, density=True, alpha=1.0, edgecolor = 'black', linewidth = 0.001)\n",
        "ax.tick_params(axis='both', which='major', labelsize=30, width = 3, length = 10)\n",
        "ax.tick_params(axis='both', which='minor', labelsize=30, width = 3, length = 5)\n",
        "\n",
        "ax.spines[['right', 'top']].set_visible(False)\n",
        "ax.spines['left'].set_linewidth(3)\n",
        "ax.spines['bottom'].set_linewidth(3)\n",
        "ax.set_ylabel('Density',fontsize=40)\n",
        "ax.set_xlabel('Gradient error',fontsize=40)\n",
        "\n",
        "ax_box = plt.axes([0.6, 0.6, 0.3, 0.3])\n",
        "(osm, osr), (slope, intercept, r) = stats.probplot(noiseList, dist=stats.norm, plot=None)\n",
        "min_x = osm[0]\n",
        "max_x = (osr[-1] - intercept) / slope\n",
        "x = np.linspace(min_x, max_x, 100)\n",
        "ax_box.plot(x, slope*x + intercept, color='r', alpha=.8, linewidth=2)\n",
        "ax_box.scatter(osm, osr, s=8, alpha=1, facecolors='none', edgecolors='b')\n",
        "\n",
        "ax_box.set_xlim([min_x, max_x])\n",
        "ax_box.set_ylim([slope*min_x + intercept, slope*max_x + intercept])\n",
        "ax_box.get_xaxis().set_visible(False)\n",
        "ax_box.get_yaxis().set_visible(False)\n",
        "ax_box.spines[['left', 'right', 'top', 'bottom']].set_linewidth(3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(fig_dir + '/heavy_tail_noise_layer{}_N{}_qq.jpg'.format(n_layer, N), dpi=300, bbox_inches=\"tight\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
