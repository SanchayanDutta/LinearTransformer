{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7soBJnRez5fP",
        "outputId": "f72ef2f7-2a16-4488-b4e6-7c50e76e6652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with adam optimizer, Learning Rate: 0.001\n",
            "Epoch 0, Loss: 0.37667232751846313\n",
            "Epoch 100, Loss: 0.16827353835105896\n",
            "Epoch 200, Loss: 0.1396411508321762\n",
            "Epoch 300, Loss: 0.13879340887069702\n",
            "Training with adam optimizer, Learning Rate: 0.01\n",
            "Epoch 0, Loss: 0.2456808239221573\n",
            "Epoch 100, Loss: 0.052687838673591614\n",
            "Epoch 200, Loss: 0.027845948934555054\n",
            "Epoch 300, Loss: 0.013793528079986572\n",
            "Training with adam optimizer, Learning Rate: 0.05\n",
            "Epoch 0, Loss: 0.26487094163894653\n",
            "Epoch 100, Loss: 0.03570482134819031\n",
            "Epoch 200, Loss: 0.028097547590732574\n",
            "Epoch 300, Loss: 0.008483467623591423\n",
            "Training with adam optimizer, Learning Rate: 0.1\n",
            "Epoch 0, Loss: 0.5550039410591125\n",
            "Epoch 100, Loss: 0.19777850806713104\n",
            "Epoch 200, Loss: 0.1297670602798462\n",
            "Epoch 300, Loss: 0.11323829740285873\n",
            "Optimal Learning Rate for adam: 0.01\n",
            "Training with sgd optimizer, Learning Rate: 0.001\n",
            "Epoch 0, Loss: 0.9390906095504761\n",
            "Epoch 100, Loss: 1.0138301849365234\n",
            "Epoch 200, Loss: 0.9242967963218689\n",
            "Epoch 300, Loss: 1.0003567934036255\n",
            "Training with sgd optimizer, Learning Rate: 0.01\n",
            "Epoch 0, Loss: 0.3734271228313446\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)  # PyTorch\n",
        "\n",
        "    # Ensures that PyTorch gets the same random numbers for CUDA as well, if using GPU.\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def attention(P,Q,Z, activation = None):\n",
        "    B= Z.shape[0]\n",
        "    N = Z.shape[1]-1\n",
        "    d = Z.shape[2]-1\n",
        "    A = torch.eye(N+1).to(device)\n",
        "    A[N,N] = 0\n",
        "    Attn = torch.einsum('BNi, ij, BMj -> BNM', (Z,Q,Z))\n",
        "    if activation is not None:\n",
        "        Attn = activation(Attn)\n",
        "    key = torch.einsum('ij, BNj -> BNi', (P,Z))\n",
        "    Output = torch.einsum('BNM,ML, BLi -> BNi', (Attn,A,key))\n",
        "    return Output /N\n",
        "\n",
        "\n",
        "\n",
        "class Transformer_MLP(nn.Module):\n",
        "    def __init__(self, n_layer, n_head, d, var, hidden_dim):\n",
        "        super(Transformer_MLP, self).__init__()\n",
        "        self.register_parameter('allparam', torch.nn.Parameter(torch.zeros(n_layer, n_head, 2, d+1, d+1)))\n",
        "        with torch.no_grad():\n",
        "            self.allparam.normal_(0,var)\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.mlp = nn.Sequential(nn.Linear(d, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim, d))\n",
        "\n",
        "    def forward(self, Z):\n",
        "        Z[:,:,:-1] = self.mlp(Z[:,:,:-1].clone())\n",
        "        for i in range(self.n_layer):\n",
        "            Zi = Z\n",
        "            residues = 0\n",
        "            # the forwarad map of each layer is given by F(Z) = Z + attention(Z)\n",
        "            for j in range(self.n_head):\n",
        "                Pij = self.allparam[i,j,0,:,:]\n",
        "                Qij = self.allparam[i,j,1,:,:]\n",
        "                residues = residues + attention(Pij,Qij,Zi)\n",
        "            Z = Zi + residues\n",
        "        return Z\n",
        "\n",
        "# evaluate the loss of model, given data (Z,y)\n",
        "def in_context_loss(model, Z, y):\n",
        "    N = Z.shape[1]-1\n",
        "    d = Z.shape[2]-1\n",
        "    output = model(Z)\n",
        "    diff = output[:,N,d]+y\n",
        "    loss = ((diff)**2).mean()\n",
        "    return loss\n",
        "\n",
        "def generate_data_mlp(N, B, d, randomMLP):\n",
        "    # Generate random input data\n",
        "    X = torch.FloatTensor(B, N, d).normal_(0, 1).to(device)\n",
        "    X_test = torch.FloatTensor(B, 1, d).normal_(0, 1).to(device)\n",
        "\n",
        "    # Additional transformations if mode is 'sphere' or 'gamma' [Similar to the existing generate_data function]\n",
        "\n",
        "    X_MLP = randomMLP(X.view(-1, d)).view(B, N, d)\n",
        "    X_test_MLP = randomMLP(X_test.view(-1, d)).view(B, 1, d)\n",
        "\n",
        "    W = torch.FloatTensor(B, d).normal_(0,1).cuda()\n",
        "    y = torch.einsum('bi,bni->bn', (W, X_MLP)).unsqueeze(2)\n",
        "    y_zero = torch.zeros(B,1,1).cuda()\n",
        "    y_test = torch.einsum('bi,bni->bn', (W, X_test_MLP)).squeeze(1)\n",
        "    X_comb= torch.cat([X,X_test],dim=1)\n",
        "    y_comb= torch.cat([y,y_zero],dim=1)\n",
        "    Z= torch.cat([X_comb,y_comb],dim=2)\n",
        "\n",
        "    return Z, y_test\n",
        "# Setup\n",
        "N=20\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_layer = 3  # number of layers of transformer\n",
        "d = 5        # dimension for the data\n",
        "n_head = 1   # 1-headed attention\n",
        "B = 2000     # minibatch size for sine data\n",
        "var = 0.0001 # initializations scale of transformer parameter\n",
        "clip =1\n",
        "\n",
        "\n",
        "tuning_epochs = 400\n",
        "max_iters = 2000  # Number of Iterations to run\n",
        "hidden_dim = 10\n",
        "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
        "\n",
        "# Function to run training\n",
        "def train_transformer(optimizer_name, learning_rate, max_epochs):\n",
        "    randomMLP = nn.Sequential(nn.Linear(d, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim, d)).to(device)\n",
        "    for para in randomMLP.parameters():\n",
        "        para.requires_grad = False\n",
        "    # Initialize model and optimizer\n",
        "\n",
        "    # Instantiate model\n",
        "    model = Transformer_MLP(n_layer, n_head, d, var, 3* hidden_dim).to(device)\n",
        "\n",
        "    # Wrap the model for DataParallel\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    if optimizer_name == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer_name == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99))\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(max_epochs):\n",
        "        # Generate data\n",
        "        Z, y = generate_data_mlp(N=N, B=B, d=d, randomMLP=randomMLP)\n",
        "\n",
        "        # Move data to the correct device\n",
        "        Z, y = Z.to(device), y.to(device)\n",
        "\n",
        "        # Training step\n",
        "        optimizer.zero_grad()\n",
        "        loss = in_context_loss(model, Z, y)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Record loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Optionally print progress\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "    return losses\n",
        "\n",
        "# Tuning learning rate\n",
        "\n",
        "optimal_lr = {}\n",
        "for optimizer_name in ['adam','sgd']:\n",
        "    best_lr = None\n",
        "    best_loss = float('inf')\n",
        "    for lr in learning_rates:\n",
        "        print(f\"Training with {optimizer_name} optimizer, Learning Rate: {lr}\")\n",
        "        losses = train_transformer(optimizer_name, lr, tuning_epochs)\n",
        "        avg_loss = np.mean(losses[-20:])  # Average loss over last epochs\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            best_lr = lr\n",
        "    optimal_lr[optimizer_name] = best_lr\n",
        "    print(f\"Optimal Learning Rate for {optimizer_name}: {best_lr}\")\n",
        "\n",
        "num_runs = 5\n",
        "final_losses = {}\n",
        "for optimizer_name, lr in optimal_lr.items():\n",
        "    print(f\"Extended Training with {optimizer_name} optimizer, Learning Rate: {lr}\")\n",
        "    all_losses = []\n",
        "    for seed in range(num_runs):\n",
        "        set_random_seed(seed)  # Function to set the random seed, define it as needed\n",
        "        losses = train_transformer(optimizer_name, lr, max_iters)\n",
        "        all_losses.append(losses)\n",
        "\n",
        "    # Calculate the mean and standard deviation of the losses\n",
        "    mean_losses = np.mean(all_losses, axis=0)\n",
        "    std_losses = np.std(all_losses, axis=0)\n",
        "    final_losses[optimizer_name] = (mean_losses, std_losses)\n",
        "\n",
        "# Plotting losses with shaded error bar\n",
        "plt.figure(figsize=(10, 6))\n",
        "epochs = range(len(mean_losses))  # Assuming all runs have the same number of epochs\n",
        "for optimizer_name, (mean_losses, std_losses) in final_losses.items():\n",
        "    plt.plot(epochs, mean_losses, label=f'{optimizer_name} LR={optimal_lr[optimizer_name]}')\n",
        "    plt.fill_between(epochs, mean_losses - std_losses, mean_losses + std_losses, alpha=0.3)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}