{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7soBJnRez5fP",
        "outputId": "11b2170e-ca9b-4917-b1d4-801a4f1bf882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with adam optimizer, Learning Rate: 0.001\n",
            "Epoch 0, Loss: 0.6350594758987427\n",
            "Epoch 100, Loss: 0.158135786652565\n",
            "Epoch 200, Loss: 0.12263938039541245\n",
            "Epoch 300, Loss: 0.1388525813817978\n",
            "Epoch 400, Loss: 0.08116383850574493\n",
            "Epoch 500, Loss: 0.040455229580402374\n",
            "Epoch 600, Loss: 0.03448481485247612\n",
            "Epoch 700, Loss: 0.02776438742876053\n",
            "Epoch 800, Loss: 0.024515120312571526\n",
            "Epoch 900, Loss: 0.025764331221580505\n",
            "Training with adam optimizer, Learning Rate: 0.01\n",
            "Epoch 0, Loss: 0.5359437465667725\n",
            "Epoch 100, Loss: 0.11812637001276016\n",
            "Epoch 200, Loss: 0.07110867649316788\n",
            "Epoch 300, Loss: 0.04054254665970802\n",
            "Epoch 400, Loss: 0.03940672427415848\n",
            "Epoch 500, Loss: 0.02384348027408123\n",
            "Epoch 600, Loss: 0.01964680105447769\n",
            "Epoch 700, Loss: 0.012951521202921867\n",
            "Epoch 800, Loss: 0.010303180664777756\n",
            "Epoch 900, Loss: 0.00881048571318388\n",
            "Training with adam optimizer, Learning Rate: 0.05\n",
            "Epoch 0, Loss: 0.48296472430229187\n",
            "Epoch 100, Loss: 0.2387392669916153\n",
            "Epoch 200, Loss: 0.0917467474937439\n",
            "Epoch 300, Loss: 0.10052630305290222\n",
            "Epoch 400, Loss: 0.051679957658052444\n",
            "Epoch 500, Loss: 0.06667027622461319\n",
            "Epoch 600, Loss: 0.033562470227479935\n",
            "Epoch 700, Loss: 0.03127450868487358\n",
            "Epoch 800, Loss: 0.024837909266352654\n",
            "Epoch 900, Loss: 0.02526077814400196\n",
            "Training with adam optimizer, Learning Rate: 0.1\n",
            "Epoch 0, Loss: 0.5496828556060791\n",
            "Epoch 100, Loss: 0.1829128861427307\n",
            "Epoch 200, Loss: 0.23216131329536438\n",
            "Epoch 300, Loss: 0.2040347456932068\n",
            "Epoch 400, Loss: 0.17073389887809753\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def clip_and_step(allparam, optimizer, clip_r = None):\n",
        "    norm_p=None\n",
        "    grad_all = allparam.grad\n",
        "    if clip_r is not None:\n",
        "        norm_p = grad_all.norm().item()\n",
        "        if norm_p > clip_r:\n",
        "            grad_all.mul_(clip_r/norm_p)\n",
        "    optimizer.step()\n",
        "    return norm_p\n",
        "\n",
        "def attention(P,Q,Z, activation = None):\n",
        "    B= Z.shape[0]\n",
        "    N = Z.shape[1]-1\n",
        "    d = Z.shape[2]-1\n",
        "    P_full =  torch.cat([P,torch.zeros(1,d).to(device)],dim=0)\n",
        "    P_full =  torch.cat([P_full,torch.zeros(d+1,1).to(device)],dim=1)\n",
        "    P_full[d,d] = 1\n",
        "    Q_full = torch.cat([Q, torch.zeros(1,d).to(device)],dim=0)\n",
        "    Q_full = torch.cat([Q_full, torch.zeros(d+1,1).to(device)],dim=1)\n",
        "    A = torch.eye(N+1).to(device)\n",
        "    A[N,N] = 0\n",
        "    Attn = torch.einsum('BNi, ij, BMj -> BNM', (Z,Q_full,Z))\n",
        "    if activation is not None:\n",
        "        Attn = activation(Attn)\n",
        "    key = torch.einsum('ij, BNj -> BNi', (P_full,Z))\n",
        "    Output = torch.einsum('BNM,ML, BLi -> BNi', (Attn,A,key))\n",
        "    return Output /N\n",
        "\n",
        "\n",
        "class Transformer_F(nn.Module):\n",
        "    def __init__(self, n_layer, n_head, d, var):\n",
        "        super(Transformer_F, self).__init__()\n",
        "        self.register_parameter('allparam', torch.nn.Parameter(torch.zeros(n_layer, n_head, 2, d, d)))\n",
        "        with torch.no_grad():\n",
        "            self.allparam.normal_(0,var)\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "\n",
        "    def forward(self, Z):\n",
        "        for i in range(self.n_layer):\n",
        "            Zi = Z\n",
        "            residues = 0\n",
        "            # the forwarad map of each layer is given by F(Z) = Z + attention(Z)\n",
        "            for j in range(self.n_head):\n",
        "                Pij = self.allparam[i,j,0,:,:]\n",
        "                Qij = self.allparam[i,j,1,:,:]\n",
        "                residues = residues + attention(Pij,Qij,Zi)\n",
        "            Z = Zi + residues\n",
        "        return Z\n",
        "\n",
        "    #enforces top-left-dxd-block sparsity on p\n",
        "    def zero_p(self):\n",
        "        for i in range(self.n_layer):\n",
        "            for j in range(self.n_head):\n",
        "                with torch.no_grad():\n",
        "                    self.allparam[i,j,0,:,:].zero_()\n",
        "\n",
        "class Transformer_MLP_freeze(nn.Module):\n",
        "    def __init__(self, n_layer, n_head, d, var, randomMLP):\n",
        "        super(Transformer_MLP_freeze, self).__init__()\n",
        "        self.register_parameter('allparam', torch.nn.Parameter(torch.zeros(n_layer, n_head, 2, d, d)))\n",
        "        with torch.no_grad():\n",
        "            self.allparam.normal_(0,var)\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.randomMLP = randomMLP\n",
        "\n",
        "    def forward(self, Z):\n",
        "        Z[:,:,:-1] = self.randomMLP(Z[:,:,:-1])\n",
        "        for i in range(self.n_layer):\n",
        "            Zi = Z\n",
        "            residues = 0\n",
        "            # the forwarad map of each layer is given by F(Z) = Z + attention(Z)\n",
        "            for j in range(self.n_head):\n",
        "                Pij = self.allparam[i,j,0,:,:]\n",
        "                Qij = self.allparam[i,j,1,:,:]\n",
        "                residues = residues + attention(Pij,Qij,Zi)\n",
        "            Z = Zi + residues\n",
        "        return Z\n",
        "\n",
        "class Transformer_MLP(nn.Module):\n",
        "    def __init__(self, n_layer, n_head, d, var, hidden_dim):\n",
        "        super(Transformer_MLP, self).__init__()\n",
        "        self.register_parameter('allparam', torch.nn.Parameter(torch.zeros(n_layer, n_head, 2, d, d)))\n",
        "        with torch.no_grad():\n",
        "            self.allparam.normal_(0,var)\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.mlp = nn.Sequential(nn.Linear(d, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim, d))\n",
        "\n",
        "    def forward(self, Z):\n",
        "        Z[:,:,:-1] = self.mlp(Z[:,:,:-1].clone())\n",
        "        for i in range(self.n_layer):\n",
        "            Zi = Z\n",
        "            residues = 0\n",
        "            # the forwarad map of each layer is given by F(Z) = Z + attention(Z)\n",
        "            for j in range(self.n_head):\n",
        "                Pij = self.allparam[i,j,0,:,:]\n",
        "                Qij = self.allparam[i,j,1,:,:]\n",
        "                residues = residues + attention(Pij,Qij,Zi)\n",
        "            Z = Zi + residues\n",
        "        return Z\n",
        "\n",
        "# evaluate the loss of model, given data (Z,y)\n",
        "def in_context_loss(model, Z, y):\n",
        "    N = Z.shape[1]-1\n",
        "    d = Z.shape[2]-1\n",
        "    output = model(Z)\n",
        "    diff = output[:,N,d]+y\n",
        "    loss = ((diff)**2).mean()\n",
        "    return loss\n",
        "\n",
        "def generate_data_mlp(N, B, d, randomMLP):\n",
        "    # Generate random input data\n",
        "    X = torch.FloatTensor(B, N, d).normal_(0, 1).to(device)\n",
        "    X_test = torch.FloatTensor(B, 1, d).normal_(0, 1).to(device)\n",
        "\n",
        "    # Additional transformations if mode is 'sphere' or 'gamma' [Similar to the existing generate_data function]\n",
        "\n",
        "    X_MLP = randomMLP(X.view(-1, d)).view(B, N, d)\n",
        "    X_test_MLP = randomMLP(X_test.view(-1, d)).view(B, 1, d)\n",
        "\n",
        "    W = torch.FloatTensor(B, d).normal_(0,1).cuda()\n",
        "    y = torch.einsum('bi,bni->bn', (W, X_MLP)).unsqueeze(2)\n",
        "    y_zero = torch.zeros(B,1,1).cuda()\n",
        "    y_test = torch.einsum('bi,bni->bn', (W, X_test_MLP)).squeeze(1)\n",
        "    X_comb= torch.cat([X,X_test],dim=1)\n",
        "    y_comb= torch.cat([y,y_zero],dim=1)\n",
        "    Z= torch.cat([X_comb,y_comb],dim=2)\n",
        "\n",
        "    return Z, y_test\n",
        "# Setup\n",
        "N=20\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_layer = 3  # number of layers of transformer\n",
        "d = 5        # dimension for the data\n",
        "n_head = 1   # 1-headed attention\n",
        "B = 1000     # minibatch size for sine data\n",
        "var = 0.0001 # initializations scale of transformer parameter\n",
        "clip_r =1\n",
        "\n",
        "\n",
        "tuning_epochs = 1000\n",
        "max_iters = 4000  # Number of Iterations to run\n",
        "hidden_dim = 5\n",
        "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
        "\n",
        "# Function to run training\n",
        "def train_transformer(optimizer_name, learning_rate, max_epochs):\n",
        "    randomMLP = nn.Sequential(nn.Linear(d, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim, d)).to(device)\n",
        "    for para in randomMLP.parameters():\n",
        "        para.requires_grad = False\n",
        "    # Initialize model and optimizer\n",
        "\n",
        "    # Instantiate model\n",
        "    model = Transformer_MLP(n_layer, n_head, d, var, hidden_dim).to(device)\n",
        "    #model = Transformer_MLP_freeze(n_layer, n_head, d, var, randomMLP).to(device)\n",
        "\n",
        "    # Wrap the model for DataParallel\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    if optimizer_name == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer_name == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99))\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(max_epochs):\n",
        "        # Generate data\n",
        "        Z, y = generate_data_mlp(N=N, B=B, d=d, randomMLP=randomMLP)\n",
        "\n",
        "        # Move data to the correct device\n",
        "        Z, y = Z.to(device), y.to(device)\n",
        "\n",
        "        # Training step\n",
        "        optimizer.zero_grad()\n",
        "        loss = in_context_loss(model, Z, y)\n",
        "        loss.backward()\n",
        "        norms = clip_and_step(model.allparam, optimizer, clip_r=clip_r)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Record loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Optionally print progress\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "    return losses\n",
        "\n",
        "# Tuning learning rate\n",
        "\n",
        "optimal_lr = {}\n",
        "for optimizer_name in ['adam','sgd']:\n",
        "    best_lr = None\n",
        "    best_loss = float('inf')\n",
        "    for lr in learning_rates:\n",
        "        print(f\"Training with {optimizer_name} optimizer, Learning Rate: {lr}\")\n",
        "        losses = train_transformer(optimizer_name, lr, tuning_epochs)\n",
        "        avg_loss = np.mean(losses[-20:])  # Average loss over last epochs\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            best_lr = lr\n",
        "    optimal_lr[optimizer_name] = best_lr\n",
        "    print(f\"Optimal Learning Rate for {optimizer_name}: {best_lr}\")\n",
        "\n",
        "\n",
        "# Extended training with optimal learning rates\n",
        "final_losses = {}\n",
        "for optimizer_name, lr in optimal_lr.items():\n",
        "    print(f\"Extended Training with {optimizer_name} optimizer, Learning Rate: {lr}\")\n",
        "    losses = train_transformer(optimizer_name, lr, max_iters)\n",
        "    final_losses[optimizer_name] = losses\n",
        "\n",
        "# Plotting losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "for optimizer_name, losses in final_losses.items():\n",
        "    plt.plot(losses, label=f'{optimizer_name} LR={optimal_lr[optimizer_name]}')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}