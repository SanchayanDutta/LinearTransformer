{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3fcfaf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "##############################################################################################################\n",
    "# Trains a linear Transformer with 1,2,3,4 layers\n",
    "# Plots the test loss of trained Transformer against 1,2,3,4 steps of gradient descent (with and without preconditioning)\n",
    "##############################################################################################################\n",
    "\n",
    "#use cuda if available, else use cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#torch.cuda.set_device(1)\n",
    "# import the model and some useful functions\n",
    "from linear_transformer import Transformer_F, attention, generate_data, in_context_loss, generate_data_inplace\n",
    "\n",
    "# set up some print options\n",
    "np.set_printoptions(precision = 2, suppress = True)\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "#begin logging\n",
    "cur_dir = 'log' \n",
    "os.makedirs(cur_dir, exist_ok=True)\n",
    "#f = open(cur_dir + '/rotation.log', \"a\", 1)\n",
    "#sys.stdout = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9700bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up problem parameters\n",
    "\n",
    "lr = 0.01\n",
    "clip_r = 0.01\n",
    "alg = 'adam'\n",
    "mode = 'normal'\n",
    "\n",
    "n_layer = 4  # number of layers of transformer\n",
    "N = 20     # context length\n",
    "d = 5        # dimension\n",
    "\n",
    "\n",
    "n_head = 1  # 1-headed attention\n",
    "B = 20000  # 1000 minibatch size\n",
    "var = 0.0001  # initializations scale of transformer parameter\n",
    "shape_k = 0.1  # shape_k: parameter for Gamma distributed covariates\n",
    "max_iters = 20000  # Number of Iterations to run\n",
    "hist_stride = 1  # stride for saved model paramters in `train.ipynb'\n",
    "stride = 100\n",
    "\n",
    "# a convenience function for taking a step and clipping\n",
    "def clip_and_step(allparam, optimizer, clip_r = None):\n",
    "    norm_p=None\n",
    "    grad_all = allparam.grad\n",
    "    if clip_r is not None:\n",
    "        for l in range(grad_all.shape[0]):\n",
    "            for h in range(grad_all.shape[1]):\n",
    "                for t in range(grad_all.shape[2]):\n",
    "                    norm_p = grad_all[l,h,t,:,:].norm().item()\n",
    "                    if norm_p > clip_r:\n",
    "                        grad_all[l,h,t,:,:].mul_(clip_r/norm_p)\n",
    "    optimizer.step()\n",
    "    return norm_p\n",
    "\n",
    "#format for saving run data\n",
    "filename_format = '/variable_L_hist_{}_{}_{}.pth'\n",
    "n_layers = [1,2,3,4]  # number of layers of transformer\n",
    "seeds=[0,1,2,3,4]\n",
    "keys = []\n",
    "for s in seeds:\n",
    "    for n_layer in n_layers:\n",
    "        keys.append((s,n_layer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d0ea4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    sd = key[0]\n",
    "    n_layer = key[1]\n",
    "    filename = cur_dir + filename_format.format(n_layer, N, sd)\n",
    "    print(key)\n",
    "    \n",
    "    prob_seed = sd\n",
    "    opt_seed = sd\n",
    "    \n",
    "    hist = []\n",
    "    \n",
    "    #set seed and initialize model\n",
    "    torch.manual_seed(opt_seed)\n",
    "    model = Transformer_F(n_layer, 1, d, var)\n",
    "    model.to(device)\n",
    "    #initialize algorithm. Important: set beta = 0.9 for adam, 0.999 is very slow\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.99, 0.9), weight_decay=0)\n",
    "    \n",
    "    # set seed\n",
    "    # sample random rotation matrix\n",
    "    # initialize initial training batch\n",
    "    np.random.seed(prob_seed)\n",
    "    torch.manual_seed(prob_seed)\n",
    "    gaus = torch.FloatTensor(5,5).uniform_(-1,1).cuda()\n",
    "    U = torch.linalg.svd (gaus)[0].cuda()\n",
    "    D = torch.diag(torch.FloatTensor([1,1,1/2,1/4,1])).cuda()\n",
    "    Z, y = generate_data(mode,N,d,B,shape_k, U, D)\n",
    "    Z = Z.to(device)\n",
    "    y = y.to(device)\n",
    "    for t in range(max_iters):\n",
    "        if t%4000==0 and t>1:\n",
    "            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] *0.5\n",
    "        if t%100==0:\n",
    "            Z,y = generate_data_inplace(Z, U=U, D=D)\n",
    "        start = time.time()\n",
    "        # save model parameters\n",
    "        if t%stride ==0:\n",
    "            hist.append(model.allparam.clone().detach())\n",
    "        loss = in_context_loss(model, Z, y)\n",
    "        # compute gradient, take step\n",
    "        loss.backward()\n",
    "        norms = clip_and_step(model.allparam, optimizer, clip_r=clip_r)\n",
    "        optimizer.zero_grad()\n",
    "        end=time.time()\n",
    "        if t%100 ==0 or t<5:\n",
    "            print('iter {} | Loss: {}  time: {}  gradnorm: {}'.format(t,loss.item(), end-start, norms))\n",
    "    torch.save({'hist':hist, 'U':U, 'D':D}, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "83154b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# compute test loss for trained linear Transformers\n",
    "########################################################\n",
    "loss_dict = {}\n",
    "for sd in seeds:\n",
    "    key = (sd,)\n",
    "    loss_dict[key] = torch.zeros(4)\n",
    "    for n_layer in n_layers:\n",
    "        # load parameters for given n_layer and seed\n",
    "        filename = cur_dir + filename_format.format(n_layer, N, sd)\n",
    "        hist = torch.load(filename)['hist']\n",
    "        U = torch.load(filename)['U']\n",
    "        D = torch.load(filename)['D']\n",
    "        \n",
    "        # given short(er) training steps, may have some unstable solutions\n",
    "        # on a validation set of (seed=999), find the solution with best validation\n",
    "        # loss from the last 20 runs\n",
    "        np.random.seed(999)\n",
    "        torch.manual_seed(999)\n",
    "        Z, y = generate_data(mode,N,d,B,shape_k,U,D)\n",
    "        Z = Z.to(device)\n",
    "        y = y.to(device)\n",
    "        model = Transformer_F(n_layer, n_head, d, var).to(device)\n",
    "        loss = 100\n",
    "        bestmodel = None\n",
    "        for t in range(len(hist)-20, len(hist)):\n",
    "            with torch.no_grad():\n",
    "                model.allparam.copy_(hist[t])\n",
    "            newloss = in_context_loss(model, Z, y).item()\n",
    "            if (newloss < loss):\n",
    "                loss=newloss\n",
    "                bestmodel = hist[t]\n",
    "        with torch.no_grad():\n",
    "            model.allparam.copy_(bestmodel)\n",
    "        np.random.seed(99)\n",
    "        torch.manual_seed(99)\n",
    "        Z, y = generate_data(mode,N,d,B,shape_k,U,D)\n",
    "        Z = Z.to(device)\n",
    "        y = y.to(device)    \n",
    "        #compute loss\n",
    "        loss_dict[key][n_layer-1] = in_context_loss(model, Z, y).log().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca8fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance of x steps of Gradient Descent\n",
    "def do_gd(Z,eta,numstep):\n",
    "    N = Z.shape[0]-1\n",
    "    X = Z[0:N-1,0:5]\n",
    "    Y = Z[0:N-1,5]\n",
    "    w = torch.zeros(X.shape[1]).to(device)\n",
    "    for k in range(numstep):\n",
    "        XTXw = torch.einsum('ik,ij,j->k',X,X,w)\n",
    "        XTY = torch.einsum('ik,i->k',X,Y)\n",
    "        grad = XTXw - XTY\n",
    "        w = w - eta * grad\n",
    "    return w\n",
    "\n",
    "def eval_w_instance(Z, Ytest, w):\n",
    "    N = Z.shape[0]-1\n",
    "    Xtest = Z[N,0:5]\n",
    "    prediction = torch.einsum('i,i->',w,Xtest)\n",
    "    return (Ytest - prediction)**2, prediction\n",
    "\n",
    "\n",
    "gd_loss_matrix = torch.zeros(len(seeds),4)\n",
    "\n",
    "\n",
    "for n_layer in n_layers:\n",
    "    #first find best eta\n",
    "    #load seed 1 for U,D matrices\n",
    "    sd = 1\n",
    "    best_loss = 10000\n",
    "    best_eta = 0\n",
    "    numstep = n_layer\n",
    "    # load UD matrices\n",
    "    filename = cur_dir + filename_format.format(n_layer, N, sd)\n",
    "    U = torch.load(filename)['U']\n",
    "    D = torch.load(filename)['D']\n",
    "    #generate test data using seed 999\n",
    "    np.random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "    Z, y = generate_data(mode,N,d,B,shape_k,U,D)\n",
    "    Z = Z.to(device)\n",
    "    y = y.to(device)\n",
    "    #done generating data \n",
    "    \n",
    "    for eta in [0.008, 0.01, 0.02, 0.04, 0.08, 0.16]:\n",
    "        ### start of evaluate mean loss ###\n",
    "        total_loss = 0\n",
    "        for i in range(5000):\n",
    "            Zi = Z[i,:,:]\n",
    "            Ytesti = y[i]\n",
    "            w = do_gd(Zi,eta,numstep)\n",
    "            gd_loss, gd_pred = eval_w_instance(Zi, Ytesti, w)\n",
    "            total_loss = total_loss + gd_loss\n",
    "        mean_loss = total_loss / 5000\n",
    "        ### end of evaluate mean loss ###\n",
    "        print('eta: {}, loss: {}'.format(eta, mean_loss))\n",
    "        if (mean_loss < best_loss):\n",
    "            best_eta = eta\n",
    "            best_loss = mean_loss\n",
    "    print('best eta: {} for n_layer={}'.format(best_eta, n_layer))\n",
    "    \n",
    "    #now do actual evaluation\n",
    "    for sd in seeds:\n",
    "        opt_seed = sd\n",
    "        \n",
    "        filename = cur_dir + filename_format.format(n_layer, N, sd)\n",
    "        U = torch.load(filename)['U']\n",
    "        D = torch.load(filename)['D']\n",
    "        #generate test data\n",
    "        torch.manual_seed(sd)\n",
    "        Z, y = generate_data(mode,N,d,B,shape_k,U,D)\n",
    "        Z = Z.to(device)\n",
    "        y = y.to(device)\n",
    "        #done generating data \n",
    "        eta = best_eta\n",
    "        ### start of evaluate mean loss ###\n",
    "        total_loss = 0\n",
    "        for i in range(Z.shape[0]):\n",
    "            Zi = Z[i,:,:]\n",
    "            Ytesti = y[i]\n",
    "            w = do_gd(Zi,eta,numstep)\n",
    "            gd_loss, gd_pred = eval_w_instance(Zi, Ytesti, w)\n",
    "            total_loss = total_loss + gd_loss\n",
    "        mean_loss = total_loss / Z.shape[0]\n",
    "        gd_loss_matrix[sd,n_layer-1] = mean_loss\n",
    "        \n",
    "#compute mean and std of log test loss for plotting\n",
    "gd_loss_mean = gd_loss_matrix.log().mean(dim=0)\n",
    "gd_loss_std = gd_loss_matrix.log().var(dim=0)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d80e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_preconditioned_gd(Z,eta,numstep,U,D):\n",
    "    N = Z.shape[0]-1\n",
    "    X = Z[0:N-1,0:5]\n",
    "    Y = Z[0:N-1,5]\n",
    "    w = torch.zeros(X.shape[1]).to(device)\n",
    "    X = torch.einsum('ij, jk, Nk -> Ni', (torch.inverse(D),U.t(),X))\n",
    "    for k in range(numstep):\n",
    "        XTXw = torch.einsum('ik,ij,j->k',X,X,w)\n",
    "        XTY = torch.einsum('ik,i->k',X,Y)\n",
    "        grad = XTXw - XTY\n",
    "        w = w - eta * grad\n",
    "    return w\n",
    "\n",
    "def eval_w_instance_precon(Z, Ytest, w, U, D):\n",
    "    N = Z.shape[0]-1\n",
    "    Xtest = Z[N,0:5]\n",
    "    Xtest = torch.einsum('ij, jk, k -> i', (torch.inverse(D),U.t(),Xtest))\n",
    "    prediction = torch.einsum('i,i->',w,Xtest)\n",
    "    return (Ytest - prediction)**2, prediction\n",
    "\n",
    "\n",
    "\n",
    "pgd_loss_matrix = torch.zeros(len(seeds),4)\n",
    "\n",
    "for n_layer in n_layers:\n",
    "    #first find best eta\n",
    "    #load seed 1 for U,D matrices\n",
    "    sd = 1\n",
    "    best_loss = 10000\n",
    "    best_eta = 0\n",
    "    numstep = n_layer\n",
    "    # load UD matrices\n",
    "    filename = cur_dir + filename_format.format(n_layer, N, sd)\n",
    "    U = torch.load(filename)['U'].to(device)\n",
    "    D = torch.load(filename)['D'].to(device)\n",
    "    #generate test data using seed 999\n",
    "    np.random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "    Z, y = generate_data(mode,N,d,B,shape_k,U,D)\n",
    "    Z = Z.to(device)\n",
    "    y = y.to(device)\n",
    "    #done generating data \n",
    "    \n",
    "    for eta in [0.001, 0.002, 0.004, 0.008, 0.01, 0.02, 0.04, 0.08, 0.16]:\n",
    "        ### start of evaluate mean loss ###\n",
    "        total_loss = 0\n",
    "        for i in range(5000):\n",
    "            Zi = Z[i,:,:]\n",
    "            Ytesti = y[i]\n",
    "            w = do_preconditioned_gd(Zi,eta,numstep,U,D)\n",
    "            pgd_loss, pgd_pred = eval_w_instance_precon(Zi, Ytesti, w, U, D)\n",
    "            total_loss = total_loss + pgd_loss\n",
    "        mean_loss = total_loss / 5000\n",
    "        ### end of evaluate mean loss ###\n",
    "        print('eta: {}, loss: {}'.format(eta, mean_loss))\n",
    "        if (mean_loss < best_loss):\n",
    "            best_eta = eta\n",
    "            best_loss = mean_loss\n",
    "    print('best eta: {} for n_layer={}'.format(best_eta, n_layer))\n",
    "    \n",
    "    #now do actual evaluation\n",
    "    for sd in seeds:\n",
    "        opt_seed = sd\n",
    "        \n",
    "        filename = cur_dir + filename_format.format(n_layer, N, sd)\n",
    "        U = torch.load(filename)['U'].to(device)\n",
    "        D = torch.load(filename)['D'].to(device)\n",
    "        #generate test data\n",
    "        torch.manual_seed(sd)\n",
    "        Z, y = generate_data(mode,N,d,B,shape_k,U,D)\n",
    "        Z = Z.to(device)\n",
    "        y = y.to(device)\n",
    "        #done generating data \n",
    "        eta = best_eta\n",
    "        ### start of evaluate mean loss ###\n",
    "        total_loss = 0\n",
    "        for i in range(5000):\n",
    "            Zi = Z[i,:,:]\n",
    "            Ytesti = y[i]\n",
    "            w = do_preconditioned_gd(Zi,eta,numstep,U,D)\n",
    "            pgd_loss, pgd_pred = eval_w_instance_precon(Zi, Ytesti, w, U, D)\n",
    "            total_loss = total_loss + pgd_loss\n",
    "        mean_loss = total_loss / 5000\n",
    "        pgd_loss_matrix[sd,n_layer-1] = mean_loss\n",
    "\n",
    "#compute mean and std of log test loss for plotting\n",
    "pgd_loss_mean = pgd_loss_matrix.log().mean(dim=0)\n",
    "pgd_loss_std = pgd_loss_matrix.log().var(dim=0)**0.5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# plot final test loss against N\n",
    "####################################\n",
    "\n",
    "fig_dir = 'figures' \n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1,figsize = (9, 9))\n",
    "\n",
    "losses = torch.zeros(len(seeds), len(n_layers))\n",
    "keys = loss_dict.keys()\n",
    "for idx, key in enumerate(keys):\n",
    "    losses[idx,:] = loss_dict[key]\n",
    "losses_mean = torch.mean(losses, axis=0)\n",
    "losses_std = torch.std(losses, axis=0)\n",
    "\n",
    "plt.plot(n_layers, gd_loss_mean, color='blue', label='GD')\n",
    "plt.fill_between(n_layers, gd_loss_mean - gd_loss_std, gd_loss_mean + gd_loss_std, color='blue', alpha=0.2)\n",
    "plt.plot(n_layers, pgd_loss_mean, color='green', label='Preconditioned GD')\n",
    "plt.fill_between(n_layers, pgd_loss_mean - pgd_loss_std, pgd_loss_mean + pgd_loss_std, color='green', alpha=0.2)\n",
    "ax.plot(n_layers, losses_mean, color = 'red', lw = 3, label='Linear Transformer')\n",
    "ax.fill_between(n_layers, losses_mean-losses_std, losses_mean+losses_std, color = 'red', alpha = 0.2)\n",
    "\n",
    "plt.ylabel('log(Loss)',fontsize=30)\n",
    "plt.xlabel('Number of Layers/Steps',fontsize=30)\n",
    "ax.tick_params(axis='both', which='major', labelsize=30, width = 3, length = 10)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=20, width = 3, length = 5)\n",
    "ax.legend(fontsize=24)\n",
    "#ax.set_yscale('log')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir + '/variable-L-plot.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b77440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
