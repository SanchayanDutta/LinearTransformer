{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import relu\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import gc\n",
    "from pylab import *\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from linear_transformer import Transformer_F, attention, generate_data, in_context_loss, generate_data_inplace\n",
    "\n",
    "np.set_printoptions(precision = 4, suppress = True)\n",
    "torch.set_printoptions(precision=2)\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "\n",
    "# Fixed\n",
    "n_head = 1\n",
    "d = 5\n",
    "B = 1000\n",
    "var = 0.05\n",
    "shape_k = 0.1\n",
    "\n",
    "# Number of Iterations to run\n",
    "max_iters = 10000\n",
    "hist_stride = 1\n",
    "\n",
    "# We vary the following parameters\n",
    "n_layer = 3\n",
    "mode = 'normal'\n",
    "N = 20\n",
    "seeds = [0,1,2,3,4,5]\n",
    "algos = ['sgd','adam']\n",
    "lrs = [0.02]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe output to log file\n",
    "log_dir = 'log' \n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "f = open(log_dir + '/train.log', \"a\", 1)\n",
    "sys.stdout = f\n",
    "filename_format = log_dir + '/train_layer{}_N{}_{}_{}_{}_lr{}_sd{}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-step update of (non-)clipping algotirthm\n",
    "def clip_and_step(allparam, optimizer, toclip, clip_threshold = 1.):\n",
    "    grad_all = allparam.grad\n",
    "    grad_p = grad_all\n",
    "    norm_p = grad_p.norm()\n",
    "    if toclip and norm_p > clip_threshold:\n",
    "            grad_all.mul_(clip_threshold/norm_p)\n",
    "    optimizer.step()\n",
    "    return norm_p.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Train linear transformer\n",
    "\n",
    "for alg in algos:\n",
    "    for toclip in [True]: # True means with clipping, False means without clipping\n",
    "        for lr in lrs:\n",
    "            for sd in seeds:\n",
    "                filename = filename_format.format(n_layer, N, mode, alg, toclip, lr, sd)\n",
    "                print(filename)\n",
    "                np.random.seed(sd)\n",
    "                torch.manual_seed(sd)\n",
    "                hist_list = list()\n",
    "\n",
    "                # initialize model paramter\n",
    "                model = Transformer_F(n_layer, n_head, d, var)\n",
    "                model.to(device)\n",
    "\n",
    "                # create optimizer\n",
    "                if alg == 'sgd':\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0)\n",
    "                elif alg == 'adam':\n",
    "                    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.9), weight_decay=0)\n",
    "                else: assert False\n",
    "\n",
    "                for t in range(max_iters):\n",
    "                    start = time.time()\n",
    "                    # save model parameters\n",
    "                    if t%hist_stride ==0:\n",
    "                        hist_list.append(model.allparam.clone().detach())\n",
    "\n",
    "                    #  generate a new batch of training set\n",
    "                    Z, y = generate_data(mode,N,d,B,shape_k)\n",
    "                    Z = Z.to(device)\n",
    "                    y = y.to(device)\n",
    "\n",
    "                    loss = in_context_loss(model, Z, y)\n",
    "                    loss_value = loss.item()\n",
    "                    loss.backward()\n",
    "\n",
    "                    if mode == 'sphere':\n",
    "                        clip_threshold = 0.1\n",
    "                    else:\n",
    "                        clip_threshold = 1.0\n",
    "\n",
    "                    # take optimizer step\n",
    "                    norms = clip_and_step(model.allparam, optimizer,toclip,clip_threshold)\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    end=time.time()\n",
    "                    if t%100 ==0 or t<5:\n",
    "                        print('iter {} | Loss: {}  time: {}  gradnorm: {}'.format(t,loss_value, end-start, norms))\n",
    "                \n",
    "                torch.save({'hist_list':hist_list}, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = sys.__stdout__\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
