# LinearTransformer
Pytorch code for reproducing experiments for the following papers:

[Transformers learn to implement preconditioned gradient descent for in-context learning](https://arxiv.org/abs/2306.00297)  
[Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, Suvrit Sra]

Linear attention is (maybe) all you need (to understand Transformer optimization)  
[Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Suvrit Sra, Ali Jadbabaie]
